{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note: Make sure to Restart and Run all (Kernel -> Restart and Run all) every time you modify your network before training it: Jupyter Notebook saves network weight and resumes training instead of starting it from scratch again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "from wordfreq import top_n_list\n",
    "num_words = 50000\n",
    "\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "words_to_load = 50000\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i+2 >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i + 2\n",
    "        idx2words_ft[i + 2] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "# loaded_embeddings_ft is a list of lists, mapping each word index to its corresponding vector\n",
    "# words_ft: dict, maps each word to its corresponding index, including that os <unk> and <pad>\n",
    "# idx2words_ft: dict, maps each index to the word\n",
    "\n",
    "    idx2words_ft[PAD_IDX] = '<pad>'\n",
    "    idx2words_ft[UNK_IDX] = '<unk>'\n",
    "    words_ft['<pad>'] = PAD_IDX\n",
    "    words_ft['<unk>'] = UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings_ft[1,:] = np.random.normal(size = (300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if loading was correct\n",
    "# print(type(loaded_embeddings_ft))\n",
    "# print (UNK_IDX)\n",
    "# print(loaded_embeddings_ft[words_ft['potato']])\n",
    "print(loaded_embeddings_ft[words_ft['<unk>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "snli_train = pd.read_csv(\"hw2_data/snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(\"hw2_data/snli_val.tsv\", sep='\\t')\n",
    "\n",
    "train_targets = snli_train.label\n",
    "val_targets = snli_val.label\n",
    "# Check structure of the dataframe\n",
    "snli_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets build the PyTorch DataLoader as we did in previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train premise dataset size is 100000\n",
      "Train hypothesis dataset size is 100000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for i in range(len(tokens_data)):\n",
    "        temp = []\n",
    "        for tokens in tokens_data[i].split():\n",
    "            if tokens in words_ft:\n",
    "                temp.append(words_ft[tokens])\n",
    "            else:\n",
    "                temp.append(UNK_IDX)\n",
    "                        \n",
    "        indices_data.append(temp)\n",
    "    return indices_data\n",
    "\n",
    "train_prem_data_indices = token2index_dataset(snli_train.sentence1)\n",
    "train_hypo_data_indices = token2index_dataset(snli_train.sentence2)\n",
    "val_prem_data_indices = token2index_dataset(snli_val.sentence1)\n",
    "val_hypo_data_indices = token2index_dataset(snli_val.sentence2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train premise dataset size is {}\".format(len(train_prem_data_indices)))\n",
    "print (\"Train hypothesis dataset size is {}\".format(len(train_hypo_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_prem_data_indices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices = [[train_prem_data_indices[i], train_hypo_data_indices[i]] for i in range(len(train_prem_data_indices))]\n",
    "\n",
    "val_data_indices = [[val_prem_data_indices[i], val_hypo_data_indices[i]] for i in range(len(val_hypo_data_indices))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 803, 1831, 9, 10, 6266, 7168, 4389, 18, 10, 12230, 5336, 10, 564, 7, 359, 5]\n",
      "A\n",
      "young\n",
      "girl\n",
      "in\n",
      "a\n",
      "pink\n",
      "shirt\n",
      "sitting\n",
      "on\n",
      "a\n",
      "dock\n",
      "viewing\n",
      "a\n",
      "body\n",
      "of\n",
      "water\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(train_prem_data_indices[0])\n",
    "for index in train_prem_data_indices[0]:\n",
    "    print(idx2words_ft[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  107,   348,  3303,  ...,     0,     0,     0],\n",
      "        [  107,   364,     9,  ...,     0,     0,     0],\n",
      "        [ 1897,  4389,    18,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1084,  1839, 12664,  ...,     0,     0,     0],\n",
      "        [  107,  6843,  2372,  ...,     0,     0,     0],\n",
      "        [   23,   995,     9,  ...,     0,     0,     0]])\n",
      "tensor([24, 28, 13, 10, 15, 14, 14, 16, 13, 10, 14, 12,  8, 10, 25, 11, 18,  8,\n",
      "         7, 14,  7,  7, 10, 14,  7, 13, 12, 11, 23, 14, 10, 12])\n",
      "tensor([3, 3, 2, 2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 1, 2, 3, 3, 3, 1, 1, 3, 2, 2, 1,\n",
      "        3, 1, 3, 2, 1, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list = []\n",
    "    s2_list = []\n",
    "    label_list = []\n",
    "    s1_length_list = []\n",
    "    s2_length_list = []\n",
    "    \n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        if datum[2] == \"entailment\":\n",
    "            label_list.append(1)\n",
    "        if datum[2] == \"neutral\":\n",
    "            label_list.append(2)\n",
    "        if datum[2] == \"contradiction\":\n",
    "            label_list.append(3)\n",
    "#         label_list.append(datum[2])\n",
    "        s1_length_list.append(len(datum[0][0]))\n",
    "        s2_length_list.append(len(datum[0][1]))\n",
    "\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        s1 = len(datum[0][0])\n",
    "        padded_vec = np.pad(np.array(datum[0][0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-s1)), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s1_list.append(padded_vec)\n",
    "#         print(datum)\n",
    "    for datum in batch:\n",
    "        s2 = len(datum[0][1])\n",
    "        padded_vec = np.pad(np.array(datum[0][1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-s2)), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        s2_list.append(padded_vec)\n",
    "#         print(s2_list)\n",
    "\n",
    "\n",
    "    return [torch.from_numpy(np.array(s1_list)), torch.from_numpy(np.array(s2_list)), \n",
    "            torch.LongTensor(s1_length_list), torch.LongTensor(s2_length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "\n",
    "for i, (s1, s2, s1_lengths, s2_lengths, labels) in enumerate(train_loader):\n",
    "    print (s1)\n",
    "    print (s1_lengths)\n",
    "    print (labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(4*hidden_size, num_classes) ## times 4\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size) ## time 2\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, s1, s2, s1_lengths, s2_lengths):\n",
    "        # reset hidden state\n",
    "        \n",
    "\n",
    "        s1desc_idx = np.argsort(np.array(s1_lengths))[::-1]\n",
    "        s2desc_idx = np.argsort(np.array(s2_lengths))[::-1]\n",
    "        \n",
    "#         print(np.array(s1_lengths)[desc_idx])\n",
    "        reordered_s1_lengths = (np.array(s1_lengths)[s1desc_idx])\n",
    "        reordered_s2_lengths = (np.array(s2_lengths)[s2desc_idx])\n",
    "        \n",
    "        s1order = (np.linspace(0, len(s1_lengths), len(s1_lengths), endpoint = False)).astype('int')\n",
    "        s2order = (np.linspace(0, len(s2_lengths), len(s2_lengths), endpoint = False)).astype('int')\n",
    "        \n",
    "        s1_batch_size, s1_seq_len = s1.size()\n",
    "        s2_batch_size, s2_seq_len = s2.size()\n",
    "\n",
    "        self.hidden_s1 = self.init_hidden(s1_batch_size)\n",
    "        self.hidden_s2 = self.init_hidden(s2_batch_size)\n",
    "        \n",
    "        # get embedding of characters\n",
    "        s1_embed = self.embedding(s1)\n",
    "        s2_embed = self.embedding(s2)\n",
    "        \n",
    "        # pack padded sequence\n",
    "        s1_embed = torch.nn.utils.rnn.pack_padded_sequence(s1_embed, reordered_s1_lengths, batch_first=True)\n",
    "        s2_embed = torch.nn.utils.rnn.pack_padded_sequence(s2_embed, reordered_s2_lengths, batch_first=True)\n",
    "        # fprop though RNN\n",
    "        \n",
    "        s1reorder = (s1order[s1desc_idx])\n",
    "        s2reorder = (s2order[s2desc_idx])\n",
    "        \n",
    "        reversed1 = np.argsort(s1reorder)\n",
    "        reversed2 = np.argsort(s2reorder)\n",
    "        \n",
    "        \n",
    "        s1_rnn_out, self.hidden_s1 = self.rnn(s1_embed, self.hidden_s1)\n",
    "        s2_rnn_out, self.hidden_s2 = self.rnn(s2_embed, self.hidden_s2)\n",
    "        \n",
    "        s1_rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(s1_rnn_out, batch_first=True)\n",
    "        s2_rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(s2_rnn_out, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        \n",
    "        #unsort it back\n",
    "        s1_rnn_out = s1_rnn_out[reversed1]\n",
    "        s2_rnn_out = s2_rnn_out[reversed2]\n",
    "        \n",
    "        s1_rnn_out = torch.sum(s1_rnn_out, dim=1)\n",
    "        s2_rnn_out = torch.sum(s2_rnn_out, dim=1)\n",
    "        \n",
    "        \n",
    "        rnn_out = torch.cat([s1_rnn_out, s2_rnn_out], 1)\n",
    "        #relu!!!\n",
    "        rnn_out = self.linear(rnn_out)\n",
    "        \n",
    "#         rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "#         rnn_out = torch.sum(rnn_out, dim=1)\n",
    "\n",
    "        return rnn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 30,  0,  9, 20, 31,  4, 11, 24, 12, 14, 15, 27, 25, 23,  1, 19, 16,\n",
       "        28, 26,  3, 22, 29, 21,  7, 13, 17,  2, 10,  8,  5,  6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-s1_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24, 13,  8, 12, 17,  8,  7, 10,  8, 22,  8, 17, 15,  9, 15, 14, 13,  9,\n",
      "        28, 13, 19, 10, 11, 13, 16, 14, 13, 14, 13, 11, 26, 18])\n",
      "tensor([ 7,  8,  8,  8,  8,  9,  9, 10, 10, 11, 11, 12, 13, 13, 13, 13, 13, 13,\n",
      "        14, 14, 14, 15, 15, 16, 17, 17, 18, 19, 22, 24, 26, 28])\n",
      "[ 6.  2.  5.  8. 10. 17. 13.  7. 21. 29. 22.  3. 16. 28. 19. 23.  1. 26.\n",
      " 15. 25. 27. 14. 12. 24. 11.  4. 31. 20.  9.  0. 30. 18.]\n",
      "tensor([ 9,  8,  9, 10, 11, 13, 13, 10, 15, 24, 15,  8, 13, 22, 14, 16,  8, 18,\n",
      "        13, 17, 19, 13, 13, 17, 12,  8, 28, 14, 11,  7, 26, 14])\n"
     ]
    }
   ],
   "source": [
    "print(s1_lengths)\n",
    "order = np.linspace(0, len(s1_lengths), len(s1_lengths), endpoint = False)\n",
    "sort = (np.argsort(s1_lengths))\n",
    "a = (s1_lengths[np.argsort(s1_lengths)])\n",
    "print(a)\n",
    "print(order[sort])\n",
    "print(a[order[sort]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 11 14 13 13 26 10 11  9 18 11  8 23 14 10 24 13 15  8 19  8 14 17 10\n",
      " 10 14 15  8 13 13 17 10]\n",
      "\n",
      "[26 24 23 19 18 17 17 15 15 14 14 14 14 13 13 13 13 13 11 11 11 11 10 10\n",
      " 10 10 10  9  8  8  8  8]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n",
      "\n",
      "[ 5 15 12 19  9 22 30 26 17 25  2 13 21  3  4 28 16 29 10  1  7  0 14  6\n",
      " 23 24 31  8 11 18 20 27]\n",
      "\n",
      "[21 19 10 13 14  0 23 20 27  4 18 28  2 11 22  1 16  8 29  3 30 12  5 24\n",
      " 25  9  7 31 15 17  6 26]\n",
      "\n",
      "[11 11 14 13 13 26 10 11  9 18 11  8 23 14 10 24 13 15  8 19  8 14 17 10\n",
      " 10 14 15  8 13 13 17 10]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(s1_lengths))\n",
    "print()\n",
    "desc_idx = np.argsort(np.array(s1_lengths))[::-1]\n",
    "\n",
    "print(np.array(s1_lengths)[desc_idx])\n",
    "print()\n",
    "sorted_list = (np.array(s1_lengths)[desc_idx])\n",
    "order = (np.linspace(0, len(s1_lengths), len(s1_lengths), endpoint = False)).astype('int')\n",
    "\n",
    "print(order)\n",
    "print()\n",
    "# reverse1 = np.argsort(sort)\n",
    "\n",
    "reorder = (order[desc_idx])\n",
    "print(reorder)\n",
    "print()\n",
    "\n",
    "reversed1 = np.argsort(reorder)\n",
    "print(reversed1)\n",
    "print()\n",
    "print(sorted_list[reversed1])\n",
    "# sort[reverse1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to keep in mind when using variable sized sequences in RNN in Pytorch\n",
    "\n",
    "### RNN modules accept packed sequences as inputs\n",
    "* pack_padded_sequence function packs a sequence (in Tensor format) containing padded sequences of variable length. **IMPORTANT: the sequences should be sorted by length in a decreasing order before passing to this function**\n",
    "\n",
    "* pad_packed_sequence function is an inverse operation to pack_padded_sequence. Transforms a padded sequence into a tensor of variable lenth sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/THNN/generic/ClassNLLCriterion.c:93",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fc19ef9034b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/THNN/generic/ClassNLLCriterion.c:93"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for s1, s2, s1_lengths, s2_lengths, labels in loader:\n",
    "        s1_batch, s2_batch, s1_lengths_batch, s2_lengths_batch, labels_batch = s1, s2, s1_lengths, s2_lengths, labels\n",
    "        outputs = F.softmax(model(s1_batch, s2_batch, s1_lengths_batch, s2_lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=100, hidden_size=200, num_layers=1, num_classes=3, vocab_size=len(idx2words_ft))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (s1, s2, s1_lengths, s2_lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(s1, s2, s1_lengths, s2_lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "### Implement LSTM cell instead of RNN cell. Train the model and compare the results.\n",
    "### Hint (modify init_hidden function and cell in __init__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Implement Bidirectional LSTM. You can do it very easily by adding one argument to cell when you create it.\n",
    "### For better understanding we recommend that you implement it youself by reversing a sequence and passing it to another cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "### Add max-pooling (over time) after passing through RNN instead of summing over hidden layers through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, s1, s2, s1_lengths, s2_lengths):\n",
    "        s1_batch_size, s1_seq_len = s1.size()\n",
    "        s2_batch_size, s2_seq_len = s2.size()\n",
    "\n",
    "        s1_embed = self.embedding(s1)\n",
    "        s2_embed = self.embedding(s2)\n",
    "        \n",
    "        s1_hidden = self.conv1(s1_embed.transpose(1,2)).transpose(1,2)\n",
    "        s2_hidden = self.conv1(s2_embed.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        s1_hidden = F.relu(s1_hidden.contiguous().view(-1, s1_hidden.size(-1))).view(s1_batch_size, s1_seq_len, s1_hidden.size(-1))\n",
    "        s2_hidden = F.relu(s2_hidden.contiguous().view(-1, s2_hidden.size(-1))).view(s2_batch_size, s2_seq_len, s1_hidden.size(-1))\n",
    "        \n",
    "        s1_hidden = self.conv2(s1_hidden.transpose(1,2)).transpose(1,2)\n",
    "        s2_hidden = self.conv2(s2_hidden.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        s1_hidden = F.relu(s1_hidden.contiguous().view(-1, s1_hidden.size(-1))).view(s1_batch_size, s2_seq_len, s1_hidden.size(-1))\n",
    "        s2_hidden = F.relu(s2_hidden.contiguous().view(-1, s2_hidden.size(-1))).view(s2_batch_size, s2_seq_len, s2_hidden.size(-1))\n",
    "        \n",
    "        s1_pool = nn.MaxPool1d(kernel_size = 3)\n",
    "        s2_pool = nn.MaxPool1d(kernel_size = 3)\n",
    "        \n",
    "        s1_hidden = s1_pool(s1_hidden)\n",
    "        s2_hidden = s2_pool(s2_hidden)\n",
    "        \n",
    "        hidden = torch.cat([s1_hidden, s2_hidden], 1)\n",
    "        \n",
    "        # Replace F.relu with gated activation\n",
    "        \n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "#         logits = self.linear(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important things to keep in mind when using Convolutional Nets for Language Tasks in Pytorch\n",
    "\n",
    "### Conv1d module expect input of size (batch_size, num_channels, length), where in our case input has size (batch_size, length, num_channels). Hence it is important call transpose(1,2) before passing it to convolutional layer and then reshape it back to (batch_size, length, num_channels) by calling transpose(1,2) again\n",
    "\n",
    "### Additionally we need to reshape hidden activations into 2D tensor before passing it to Relu layer by calling view(-1, hidden.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 37.1\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 41.7\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 41.4\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 38.5\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 39.3\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 41.9\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 44.7\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 45.3\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 41.5\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 42.5\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 44.6\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 41.9\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 42.9\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 45.7\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 44.9\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 44.4\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 44.3\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 45.9\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 46.9\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 47.8\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 48.4\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 48.3\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 48.9\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 49.7\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 46.6\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 47.2\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 47.6\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 46.8\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 48.5\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 49.1\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 43.6\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 48.4\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 47.4\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 50.2\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 48.0\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 46.3\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 51.1\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 51.7\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 49.4\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 50.9\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 52.2\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 52.3\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 51.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fa5bc147ef0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "default = []\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for s1, s2, s1_lengths, s2_lengths, labels in loader:\n",
    "        s1_batch, s2_batch, s1_lengths_batch, s2_lengths_batch, label_batch = s1, s2, s1_lengths, s2_lengths, labels\n",
    "        outputs = F.softmax(model(s1_batch, s2_batch, s1_lengths_batch, s2_lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=100, hidden_size=200, num_layers=2, num_classes=3, vocab_size=len(idx2words_ft))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (s1, s2, s1_lengths, s2_lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(s1, s2, s1_lengths, s2_lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            default.append([val_acc, train_acc])\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37.1, 37.65], [41.7, 39.983], [41.4, 41.766], [38.5, 41.676], [39.3, 42.128], [41.9, 43.708], [44.7, 45.526], [45.3, 44.689], [41.5, 42.694], [42.5, 44.472], [44.6, 46.633], [41.9, 42.274], [42.9, 44.986], [45.7, 47.486], [44.9, 48.054], [44.4, 47.017], [44.3, 47.13], [45.9, 47.036], [46.9, 47.569], [47.8, 49.81], [48.4, 49.725], [48.3, 49.361], [48.9, 50.33], [49.7, 51.396], [46.6, 47.781], [47.2, 49.126], [47.6, 50.175], [46.8, 48.507], [48.5, 52.47], [49.1, 51.596], [43.6, 45.149], [48.4, 49.251], [47.4, 50.224], [50.2, 52.536], [48.0, 49.171], [46.3, 48.546], [51.1, 52.607], [51.7, 53.816], [49.4, 51.921], [50.9, 53.546], [52.2, 53.948], [52.3, 53.832], [51.2, 53.264]]\n"
     ]
    }
   ],
   "source": [
    "print(default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "### Implement Gated Relu activations as well as Gated Linear activations and compare them with Relu (reference: https://arxiv.org/pdf/1612.08083.pdf )\n",
    "### Hint: Gated Relu activations are sigmoid(conv1_1(x)) * relu(conv1_2(x))\n",
    "### Hint: Gated Linear activations are sigmoid(conv1_1(x)) * conv1_2(x)\n",
    "\n",
    "### Feel free to play with other variants of gating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5:\n",
    "\n",
    "### Add max-pooling (over time) after passing through conv as well as add non-linear fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6:\n",
    "\n",
    "### Use Bag-of-Words and Bag-of-NGrams model for this task and compare it with RNN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7:\n",
    "\n",
    "### Use FastText for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
